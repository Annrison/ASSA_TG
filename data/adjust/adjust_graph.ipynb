{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 調整textGCN句子結構\n",
    "+ read\n",
    "+ `../result_MATE/{args_data}.train_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "from stanza.server import CoreNLPClient\n",
    "import stanza\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.install_corenlp()\n",
    "# stanza.download('en')\n",
    "# stanza_nlp = stanza.Pipeline('en')\n",
    "# os.environ['CORENLP_HOME'] = \"./stanza_corenlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_domain = \"REST\"\n",
    "args_data = \"yelp\"\n",
    "args_kr = \"DP+_2016\" # \"DP_2016\", \"DP+_2016\" # type of key_relation\n",
    "args_aspect = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義重要的dependency type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ DP_2016 的 prep 換成 case ; xsubj 目前找不到對應的\n",
    "    + prep 換成 case;\n",
    "    + xsubj 目前找不到對應的 \n",
    "\n",
    "+ DP+_2016 \n",
    "    + prep 換成 case;\n",
    "    + acomp => xcomp ; \n",
    "    + xsubj 目前找不到對應的 \n",
    "    + pobj => nmod (nmod範圍比較廣)\n",
    "    + poss => 文件說是 case 的一種 (需要確認 nmod:poss 和他的關係)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMB', 'DRI', 'FOD', 'LOC', 'SRV']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asp_dict = {\n",
    "        \"REST\": [\"AMB\", \"DRI\", \"FOD\", \"LOC\", \"SRV\"],\n",
    "        \"LAPTOP\": [\"SUPPORT\", \"OS\", \"DISPLAY\", \"BATTERY\", \"COMPANY\", \"MOUSE\", \"SOFTWARE\", \"KEYBOARD\"]\n",
    "    }\n",
    "\n",
    "relation_dict = {\n",
    "    \"DP_2016\": [\"amod\", \"case\", \"nsubj\", \"csubj\", \"xsubj\", \"dobj\", \"iobj\", \"conj\"], \n",
    "    \"DP+_2016\": [\"amod\", \"case\", \"nsubj\", \"csubj\", \"xsubj\", \"dobj\", \"iobj\", \"conj\", \n",
    "            \"advmod\", \"dep\", \"cop\", \"mark\", \"nsubjpass\", \"pobj\", \"acomp\", \"xcomp\", \"csubjpass\", \"poss\"],\n",
    "}\n",
    "\n",
    "\n",
    "asp_name_list = asp_dict[args_domain] # domain's aspect name\n",
    "asp_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 05:51:15 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-06-20 05:51:15 INFO: Use device: gpu\n",
      "2022-06-20 05:51:15 INFO: Loading: tokenize\n",
      "2022-06-20 05:51:19 INFO: Loading: pos\n",
      "2022-06-20 05:51:19 INFO: Loading: lemma\n",
      "2022-06-20 05:51:19 INFO: Loading: depparse\n",
      "2022-06-20 05:51:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: A\thead id: 3\thead: post\tdeprel: det\n",
      "id: 2\tword: blog\thead id: 3\thead: post\tdeprel: compound\n",
      "id: 3\tword: post\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: using\thead id: 3\thead: post\tdeprel: acl\n",
      "id: 5\tword: Stanford\thead id: 7\thead: Server\tdeprel: compound\n",
      "id: 6\tword: CoreNLP\thead id: 7\thead: Server\tdeprel: compound\n",
      "id: 7\tword: Server\thead id: 4\thead: using\tdeprel: obj\n",
      "id: 8\tword: in\thead id: 9\thead: New\tdeprel: case\n",
      "id: 9\tword: New\thead id: 7\thead: Server\tdeprel: nmod\n",
      "id: 10\tword: York\thead id: 9\thead: New\tdeprel: flat\n",
      "id: 11\tword: .\thead id: 3\thead: post\tdeprel: punct\n",
      "id: 1\tword: I\thead id: 2\thead: like\tdeprel: nsubj\n",
      "id: 2\tword: like\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: great\thead id: 4\thead: pizza\tdeprel: amod\n",
      "id: 4\tword: pizza\thead id: 2\thead: like\tdeprel: obj\n",
      "id: 5\tword: here\thead id: 2\thead: like\tdeprel: advmod\n"
     ]
    }
   ],
   "source": [
    "# # for test(看nlp有沒有正常啟動)\n",
    "text = 'A blog post using Stanford CoreNLP Server in New York.I like great pizza here'\n",
    "doc = nlp(text)\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for test\n",
    "# dl = []\n",
    "# text = 'A blog post using Stanford CoreNLP Server in New York.I like great pizza here'\n",
    "# doc = nlp(text)\n",
    "\n",
    "# i = 0 # sentence id\n",
    "# for sent in doc.sentences:\n",
    "#     for word in sent.words:\n",
    "#         dr = [i,word.id,word.text,word.head,sent.words[word.head-1].text if word.head > 0 else \"root\",word.deprel]\n",
    "#         dl.append(dr)\n",
    "#     i = i + 1\n",
    "    \n",
    "# test_df = pd.DataFrame(data=dl, columns=['sent_id','word_id','word','head_id','head_word','type'])\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析原本的文集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 先分析原本的文集，儲存成資料框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開啟 MATE 訓練資料分類 (yelp.train_out)\n",
    "result_pair = []\n",
    "f = open(f\"../../model_result/JASA_trainout/R0.train_out\", 'r')\n",
    "for line in f:\n",
    "    spline = line.split('\\t')\n",
    "    res_list = list(map(float, spline[1:len(asp_name_list)+1]))\n",
    "    result_pair.append({\"scode\": int(spline[0][2:-1]), \n",
    "                        \"aspect\": res_list.index(max(res_list)), \n",
    "                        \"text\":spline[-1].strip()[2:-1]})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料前處理\n",
    "df = pd.DataFrame(result_pair) # 49994\n",
    "df['text'] = df['text'].str.replace('^b\\'','')\n",
    "df['text'] = df['text'].str.replace('^b\"','')\n",
    "df['text'] = df['text'].str.replace(r'\\\\n\\'','', regex=True) \n",
    "df['text'] = df['text'].str.replace(r'\\\\n\"','', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44041it [45:27,  9.23it/s]"
     ]
    }
   ],
   "source": [
    "# parse sentences\n",
    "dl = []\n",
    "for index, row in tqdm(df.iterrows()):    \n",
    "    # parse sentence\n",
    "    doc = nlp(row['text'])     \n",
    "    i = 0 # sentence id\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            dr = [row['scode'],i,\n",
    "                  word.id,word.text, word.head, \n",
    "                  sent.words[word.head-1].text if word.head > 0 else \"root\", word.deprel]\n",
    "            dl.append(dr)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list to dataframe\n",
    "dependency_df = pd.DataFrame(data=dl, columns=['doc_id','sent_id','word_id','word','head_id','head_word','type'])\n",
    "# dependency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as dataframe\n",
    "# dependency_df.to_csv(r'dependency_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_df.shape # (5648250, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入解析完成的句子資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4713361, 7)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency = pd.read_csv('dependency_df.csv')\n",
    "dependency.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['root', 'det', 'obj', 'cc', 'conj', 'case', 'obl', 'nmod:poss',\n",
       "       'nsubj', 'acl:relcl', 'cop', 'advmod', 'parataxis', 'aux', 'ccomp',\n",
       "       'mark', 'xcomp', 'advcl', 'compound:prt', 'compound', 'amod',\n",
       "       'nmod', 'expl', 'obl:tmod', 'aux:pass', 'csubj', 'nsubj:pass',\n",
       "       'discourse', 'fixed', 'flat', 'det:predet', 'obl:npmod',\n",
       "       'cc:preconj', 'acl', 'appos', 'iobj', 'punct', 'nmod:tmod',\n",
       "       'nummod', 'list', 'dislocated', 'nmod:npmod', 'goeswith',\n",
       "       'vocative', 'reparandum', 'flat:foreign', 'csubj:pass', 'dep',\n",
       "       'orphan', '<PAD>'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48620"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dependency.doc_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>head_id</th>\n",
       "      <th>head_word</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4729</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>had</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  sent_id  word_id word  head_id head_word  type\n",
       "0    4729        0        1  had        0      root  root"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yet', 'another', 'five', 'star', 'review', 'for', 'this',\n",
       "       'fantastic', 'place', 'i', 'joked', 'around', 'with', 'the',\n",
       "       'owner', 'about', 'giving', 'him', 'a', 'terrible', 'so', 'he',\n",
       "       'could', 'get', 'little', 'bit', 'of', 'respite', 'from', 'all',\n",
       "       'yelpers', 'coming', 'in', 'there', 'but', 'was', 'some', 'pizza',\n",
       "       'sure', 'not', 'only', 'great', 'it', 'run', 'by', 'really',\n",
       "       'people', 'we', 'had', 'wonderful', 'conversation', 'proprietors',\n",
       "       'while', 'my', 'partner', 'and', 'devoured', 'most', 'an',\n",
       "       'entire', 'pie', 'to', 'ourselves', 'definitely', 'looking',\n",
       "       'forward', 'next', 'time', 'go'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency[dependency['doc_id']==1].word.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法一：解析並過濾句子\n",
    "字詞需要有和重要的dependency type 連線，才能留下來\n",
    "+ text：輸入的sentence\n",
    "+ relation_type：重要的dependency type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sentence\n",
    "# output adjust sentence\n",
    "def filter_sentence(text, relation_type, quiet=True):\n",
    "    key_relation = relation_dict[relation_type]\n",
    "    doc = nlp(text)\n",
    "    # for sent in doc.sentences \n",
    "    doc_list = [] # list for sentence\n",
    "    for sent in doc.sentences:\n",
    "        words = [] # all words\n",
    "        de_dict = {} # relation pair id\n",
    "        de_dict_word = {} # relation pair text\n",
    "\n",
    "        for word in sent.words:\n",
    "            key = word.deprel # dependency type        \n",
    "            head_id = word.head \n",
    "            node_id = word.id\n",
    "            head = sent.words[word.head-1].text if word.head > 0 else \"root\" # head of node\n",
    "            node = word.text # node word\n",
    "            words.append(node) # sentence\n",
    "\n",
    "            if key in key_relation:   \n",
    "                if quiet == False:\n",
    "                    print(key,\":\",node_id,node,head_id,head)\n",
    "                de_dict.setdefault(key, []).append(head_id)\n",
    "                de_dict.setdefault(key, []).append(node_id)\n",
    "\n",
    "                de_dict_word.setdefault(key, []).append(head)\n",
    "                de_dict_word.setdefault(key, []).append(node)\n",
    "\n",
    "        keep = list({x for v in de_dict.values() for x in v}) # word id to keep\n",
    "        keep_word = list({x for v in de_dict_word.values() for x in v}) # word text to keep\n",
    "        s = [words[i-1] for i in keep] # filtered sentence\n",
    "        doc_list.append(s)\n",
    "\n",
    "    sentence = [item for sublist in doc_list for item in sublist]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀進原本的資料，轉換成新的train_sumout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read ../result_MATE/yelp.train_out\n",
      "time cost 3659.9488260746\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_sumout = \"\" # adjust sentence\n",
    "f = open(f\"../result_MATE/{args_data}.train_out\", 'r')\n",
    "print(f\"read ../result_MATE/{args_data}.train_out\")\n",
    "\n",
    "for line in f:\n",
    "    sp = line.split('\\t')\n",
    "    \n",
    "    # get sentence scode\n",
    "    scode = int(sp[0][2:-1])\n",
    "    # replace string in sentence\n",
    "    text = sp[len(asp_name_list)+1].strip()\n",
    "    text = re.sub('^b\\'','',text)\n",
    "    text = re.sub('^b\"','',text)\n",
    "    text = text.replace(\"\\\\n'\", \"\")\n",
    "    text = text.replace(\"\\\\n\\\"\", \"\")\n",
    "    \n",
    "    # adjust sentence\n",
    "    text = filter_sentence(text, args_kr) \n",
    "    \n",
    "    # adjust sumout\n",
    "    train_sumout += 'b\"' + str(scode) + '\"'\n",
    "    train_sumout += '\\t' + 'b\"' + str(text) + '\"' + '\\n' \n",
    "    \n",
    "f.close()\n",
    "end = time.time()\n",
    "print(\"time cost\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as ../result_ASSA/DP+_2016.train_out\n"
     ]
    }
   ],
   "source": [
    "# save as new train_out\n",
    "savef = '../result_ASSA/{0}.train_out'.format(args_kr) # args_kr \"test\"\n",
    "fsum = open(savef, 'w') # args.trainout_name\n",
    "print('save as {}'.format(savef))\n",
    "fsum.write(train_sumout)\n",
    "fsum.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二：過濾句子的function（有解析資料框）\n",
    "字詞需要有和重要的dependency type 連線，才能留下來\n",
    "+ text：解析完的df\n",
    "+ relation_type：重要的dependency type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認一下：這幾個不在corpus裡面 {'acomp', 'csubjpass', 'dobj', 'nsubjpass', 'pobj', 'poss', 'xsubj'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 可以維持順序的方法（先不用）\n",
    "# s = []\n",
    "# doc_id = None\n",
    "# all_s = []\n",
    "# for index, row in dependency.iterrows():    \n",
    "#     temp_id = row['doc_id']    \n",
    "#     if temp_id != doc_id:\n",
    "#         all_s.append([doc_id,s])\n",
    "#         doc_id = temp_id\n",
    "#         s = []\n",
    "#     else:        \n",
    "#         if row['type'] in sel_relation:\n",
    "#             s.append(row['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_relation = relation_dict[args_kr] # DP_2016 args_kr\n",
    "filter_df = dependency[dependency['type'].isin(sel_relation)]\n",
    "filter_df = filter_df.groupby('doc_id')['word'].apply(list).reset_index(name='sentence')\n",
    "\n",
    "train_sumout = \"\"\n",
    "for index, row in filter_df.iterrows():    \n",
    "    scode = row['doc_id']    \n",
    "    text = [x for x in row['sentence'] if str(x) != 'nan']\n",
    "    text = ' '.join(text)\n",
    "    # adjust sumout\n",
    "    train_sumout += 'b\"' + str(scode) + '\"'\n",
    "    train_sumout += '\\t' + 'b\"' + str(text) + '\"' + '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as ../result_ASSA/DP+_2016.train_out\n"
     ]
    }
   ],
   "source": [
    "# save as new train_out\n",
    "savef = '../result_ASSA/{0}.train_out'.format(args_kr) # args_kr\n",
    "fsum = open(savef, 'w') # args.trainout_name\n",
    "print('save as {}'.format(savef))\n",
    "fsum.write(train_sumout)\n",
    "fsum.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 檢查：儲存的格式對不對(調整後的檔案)\n",
    "+ 讀取的檔案只有scode和調整後的句子\n",
    "+ rest 的訓練資料應該要有 49994 筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pair = []\n",
    "# openf = \"../result_ASSA/rest_R0.train_out\" # 原始句子檔案 \n",
    "openf = \"../result_ASSA/DP+_2016.train_out\" # 原始句子檔案 \n",
    "# openf = f\"../result_ASSA/{args_kr}.train_out\" # args_kr \"DP_2016\", \"DP+_2016\"\n",
    "# 開啟 MATE 訓練資料分類 (yelp.train_out)\n",
    "f = open(openf, 'r')\n",
    "for line in f:\n",
    "    sp = line.split('\\t')\n",
    "    result_pair.append({\"scode\": int(sp[0][2:-1]), \"text\":sp[-1].strip()})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49974"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_pair)\n",
    "# result_pair[7327]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取原始格式的檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證儲存的格式對不對(原檔)\n",
    "result_pair = []\n",
    "# 開啟 MATE 訓練資料分類 (yelp.train_out)\n",
    "f = open(f\"../result_MATE/yelp.train_out\", 'r')\n",
    "for line in f:\n",
    "    sp = line.split('\\t')\n",
    "    res_list = list(map(float, sp[1:len(asp_name_list)+1]))\n",
    "    result_pair.append({\"scode\": int(sp[0][2:-1]), \"aspect\": res_list.index(max(res_list)), \"text\":sp[len(asp_name_list)+1].strip()})\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看含有某個關鍵字的句子\n",
    "# keyword_list = ['food']\n",
    "# select_aspect = 1\n",
    "# df[\n",
    "#     (df['text'].str.contains('|'.join(keyword_list),regex=True, case=False)) &\n",
    "#     (df['aspect'] == select_aspect)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆解單句結構\n",
    "+ 看node,edge結構\n",
    "+ 看句型樹狀圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 樹的結構圖\n",
    "# parser = CoreNLPParser('http://localhost:9008') # 使用開在loclahost:9008的coreNLP服務\n",
    "# text = 'A blog post using Stanford CoreNLP Server. Visit www.khalidalnajjar.com for more details.'\n",
    "# parser.raw_parse(text)\n",
    "\n",
    "# fox_parsed = next(parser.raw_parse(text))\n",
    "# fox_parsed.pretty_print()\n",
    "\n",
    "\n",
    "# child 和 rdge 的結構\n",
    "# text = 'I didn\\'t tell her. Do you know her name?'\n",
    "# document = client.annotate(text)\n",
    "\n",
    "# # 示範句法結構\n",
    "# for i, sent in enumerate(document.sentence):\n",
    "    \n",
    "#     # get the constituency parse of the sentence\n",
    "#     constituency_parse = sent.parseTree # child \n",
    "#     print(constituency_parse)\n",
    "\n",
    "#     # get the dependency parse of the first sentence\n",
    "#     dependency_parse = sent.basicDependencies # edge\n",
    "#     print(dependency_parse)\n",
    "#     break # 只顯示第一句!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## related links\n",
    "minipar dependency list\n",
    "+ https://gate.ac.uk/releases/gate-7.0-build4195-ALL/doc/tao/splitch17.html\n",
    "+ GateNLP github: https://github.com/GateNLP/userguide/blob/master/parsers.tex\n",
    "    (\\subsect[sec:parsers:minipar:GR]{Grammatical Relationships})\n",
    "    \n",
    "Stanford \n",
    "+ UD v1 （目前是用第一版的） \n",
    "    + http://universaldependencies.org/docsv1/en/dep/index.html\n",
    "    + from http://universaldependencies.org/docsv1/\n",
    "        (> english > english relation)\n",
    "+ different version compare (比對Stanford新舊版本的名字)\n",
    "    + https://nlp.stanford.edu/pubs/USD_LREC14_paper_camera_ready.pdf\n",
    "\n",
    "+ old version of stanford\n",
    "    + https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf\n",
    "    + 有舊版每個類別的解釋\n",
    "\n",
    "+ 官網說明：\n",
    "    + https://nlp.stanford.edu/software/stanford-dependencies.shtml\n",
    "    + Since version 3.5.2 the Stanford Parser and Stanford CoreNLP output grammatical relations in the Universal Dependencies v1 representation by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependency type mapping\n",
    "+ stackoverflow\n",
    "+ 相似度的研究用：amod, prep, nsubj, csubj, xsubj, dobj and iobj (and conj)\n",
    "+ https://linguistics.stackexchange.com/questions/19872/mapping-minipar-dependencies-to-stanford-parser-dependencies\n",
    "+ 相似的paper \n",
    "+ http://www.mitpressjournals.org/doi/pdfplus/10.1162/coli_a_00034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "related link\n",
    "+ 官網示範\n",
    "+ https://corenlp.run/\n",
    "+ UD v2 https://universaldependencies.org/en/dep/index.html\n",
    "+ from https://universaldependencies.org/\n",
    "    (> English > documentation page > Syntax > English relations)\n",
    "\n",
    "source link\n",
    "nlp pipeline\n",
    "+ https://stanfordnlp.github.io/stanza/depparse.html\n",
    "\n",
    "dependency list\n",
    "+ stackoverflow: https://stackoverflow.com/questions/10687173/stanford-dependencies-list\n",
    "+ paper-Stanford typed dependencies manua(2008): https://downloads.cs.stanford.edu/nlp/software/dependencies_manual.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
